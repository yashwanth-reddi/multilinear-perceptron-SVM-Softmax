{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeSQKPvfxWh2",
        "outputId": "4322e235-ebde-416e-ba10-6f13b0f9ad8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3AESt75uayN8"
      },
      "outputs": [],
      "source": [
        "\"\"\"Utility functions for saving predictions for submission to Kaggle.\"\"\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def write_csv(file_path: str, y_list: np.ndarray):\n",
        "    \"\"\"Write a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        file_path: name of the file to save\n",
        "        y_list: y predictions\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "    solution_rows = [(\"id\", \"category\")] + [(i, y) for (i, y) in enumerate(y_list)]\n",
        "    with open(file_path, \"w\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerows(solution_rows)\n",
        "\n",
        "\n",
        "def output_submission_csv(output_file_path: str, y_test: np.ndarray):\n",
        "    \"\"\"Save predictions for Kaggle submission.\n",
        "\n",
        "    Parameters:\n",
        "        output_file_path: name of the file to save\n",
        "        y_test: y predictions\n",
        "    \"\"\"\n",
        "    write_csv(output_file_path, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LhX5dAjEaeQB"
      },
      "outputs": [],
      "source": [
        "\"\"\"Data preprocessing.\"\"\"\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def load_pickle(f: str) -> Any:\n",
        "    \"\"\"Load a pickle file.\n",
        "\n",
        "    Parameters:\n",
        "        f: the pickle filename\n",
        "\n",
        "    Returns:\n",
        "        the pickled data\n",
        "    \"\"\"\n",
        "    return pickle.load(f, encoding=\"latin1\")\n",
        "\n",
        "\n",
        "def load_CIFAR_batch(filename: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load a single batch of cifar data.\n",
        "\n",
        "    Parameters:\n",
        "        filename: the pickle filename\n",
        "\n",
        "    Returns:\n",
        "        the data\n",
        "        the labels\n",
        "    \"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        datadict = load_pickle(f)\n",
        "        X = datadict[\"data\"]\n",
        "        Y = datadict[\"labels\"]\n",
        "        X = X.reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype(\"float\")\n",
        "        Y = np.array(Y)\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "def load_CIFAR10(ROOT: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Load all of cifar data.\n",
        "\n",
        "    Parameters:\n",
        "        ROOT: the root directory containing the data\n",
        "\n",
        "    Returns:\n",
        "        training data\n",
        "        training labels\n",
        "        testing data\n",
        "        testing labels\n",
        "    \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for b in range(1, 6):\n",
        "        f = os.path.join(ROOT, \"data_batch_{}\".format(b))\n",
        "        X, Y = load_CIFAR_batch(f)\n",
        "        xs.append(X)\n",
        "        ys.append(Y)\n",
        "    Xtr = np.concatenate(xs)\n",
        "    Ytr = np.concatenate(ys)\n",
        "    Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, \"test_batch\"))\n",
        "    return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(\n",
        "    num_training: int = 49000,\n",
        "    num_validation: int = 1000,\n",
        "    num_test: int = 10000,\n",
        "    subtract_mean: bool = True,\n",
        "):\n",
        "    \"\"\"Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "\n",
        "    Parameters:\n",
        "        num_training: number of training images\n",
        "        num_validation: number of validation images\n",
        "        num_test: number of test images\n",
        "        subtract_mean: whether or not to normalize the data\n",
        "\n",
        "    Returns:\n",
        "        the train/val/test data and labels\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = os.path.join(\"/content/drive/MyDrive/CS747-assignment1/CS747-assignment1/cifar-10-python\", \"cifar-10-batches-py\")\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "        mean_image = np.mean(X_train, axis=0)\n",
        "        X_train -= mean_image\n",
        "        X_val -= mean_image\n",
        "        X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"y_train\": y_train,\n",
        "        \"X_val\": X_val,\n",
        "        \"y_val\": y_val,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_test\": y_test,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pOs5OrnaPIsP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "#from data_process import get_CIFAR10_data\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# from models import Perceptron, SVM, Softmax\n",
        "# from kaggle_submission import output_submission_csv\n",
        "%matplotlib inline\n",
        "\n",
        "# For auto-reloading external modules\n",
        "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FOvYs0GPIsV"
      },
      "source": [
        "# Loading CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuHIB35uPIsX"
      },
      "source": [
        "In the following cells we determine the number of images for each split and load the images.\n",
        "<br /> \n",
        "TRAIN_IMAGES + VAL_IMAGES = (0, 50000]\n",
        ", TEST_IMAGES = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Np876o_7PIsY"
      },
      "outputs": [],
      "source": [
        "# You can change these numbers for experimentation\n",
        "# For submission we will use the default values \n",
        "TRAIN_IMAGES = 40000\n",
        "VAL_IMAGES = 10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XPcoRmuLPIsa"
      },
      "outputs": [],
      "source": [
        "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES)\n",
        "X_train_CIFAR, y_train_CIFAR = data['X_train'], data['y_train']\n",
        "X_val_CIFAR, y_val_CIFAR = data['X_val'], data['y_val']\n",
        "X_test_CIFAR, y_test_CIFAR = data['X_test'], data['y_test']\n",
        "n_class_CIFAR = len(np.unique(y_test_CIFAR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otk5UDfzPIsb"
      },
      "source": [
        "Convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jOwybZqLPIsc"
      },
      "outputs": [],
      "source": [
        "X_train_CIFAR = np.reshape(X_train_CIFAR, (X_train_CIFAR.shape[0], -1))\n",
        "X_val_CIFAR = np.reshape(X_val_CIFAR, (X_val_CIFAR.shape[0], -1))\n",
        "X_test_CIFAR = np.reshape(X_test_CIFAR, (X_test_CIFAR.shape[0], -1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrVc5naPPIsd"
      },
      "source": [
        "### Get Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xywSKrKPIse"
      },
      "source": [
        "This function computes how well your model performs using accuracy as a metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tk_ZtQFdPIsf"
      },
      "outputs": [],
      "source": [
        "def get_acc(pred, y_test):\n",
        "    return np.sum(y_test == pred) / len(y_test) * 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87QaEv8xPIsg"
      },
      "source": [
        "# Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjXS7179PIsg"
      },
      "source": [
        "Perceptron has 2 hyperparameters that you can experiment with:\n",
        "- **Learning rate** - controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, but you should experiment with different values. We recommend changing the learning rate by factors of 10 and observing how the performance of the classifier changes. You should also try adding a **decay** which slowly reduces the learning rate over each epoch.\n",
        "- **Number of Epochs** - An epoch is a complete iterative pass over all of the data in the dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according to the perceptron update rule for each sample in the training set. You should try different values for the number of training epochs and report your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahv2iL8TPIsh"
      },
      "source": [
        "You will implement the Perceptron classifier in the **models/perceptron.py**\n",
        "\n",
        "The following code: \n",
        "- Creates an instance of the Perceptron classifier class \n",
        "- The train function of the Perceptron class is trained on the training data\n",
        "- We use the predict function to find the training accuracy as well as the testing accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpUDtQvXPIsi"
      },
      "source": [
        "## Train Perceptron on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "71jN6sCi2Bay"
      },
      "outputs": [],
      "source": [
        "\"\"\"Perceptron model.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from operator import add, sub\n",
        "class Perceptron:\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int):\n",
        "        \"\"\"Initialize a new classifier.\n",
        "\n",
        "        Parameters:\n",
        "            n_class: the number of classes\n",
        "            lr: the learning rate\n",
        "            epochs: the number of epochs to train for\n",
        "        \"\"\"\n",
        "        self.w = None\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.n_class = n_class\n",
        "\n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        \"\"\"Train the classifier.\n",
        "\n",
        "        Use the perceptron update rule as introduced in the Lecture.\n",
        "\n",
        "        Parameters:\n",
        "            X_train: a number array of shape (N, D) containing training data;\n",
        "                N examples with D dimensions\n",
        "            y_train: a numpy array of shape (N,) containing training labels\n",
        "        \"\"\"\n",
        "        \n",
        "        self.w = 0.001 * np.random.randn(self.n_class,X_train.shape[1])\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "          for index,row in enumerate(X_train):\n",
        "            predarray = np.dot(self.w,row)\n",
        "            predictclass = np.argmax(predarray)\n",
        "            if predictclass != y_train[index]:\n",
        "              self.w[y_train[index]] += self.lr * row\n",
        "              self.w[predictclass] -= .3 *self.lr * row \n",
        "        \n",
        "\n",
        "\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\n",
        "\n",
        "        Parameters:\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\n",
        "                N examples with D dimensions\n",
        "\n",
        "        Returns:\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\n",
        "                length N, where each element is an integer giving the predicted\n",
        "                class.\n",
        "        \"\"\"\n",
        "        return np.argmax(np.dot(self.w, X_test.T),axis=0)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QC0bn9Z8PIsj"
      },
      "outputs": [],
      "source": [
        "lr = 0.001\n",
        "n_epochs = 10\n",
        "\n",
        "percept_CIFAR = Perceptron(n_class_CIFAR, lr, n_epochs)\n",
        "percept_CIFAR.train(X_train_CIFAR, y_train_CIFAR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBBFbGzzPIsk",
        "outputId": "c5b99077-d5a6-43ac-eafb-22c598249293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training accuracy is given by: 37.587500\n"
          ]
        }
      ],
      "source": [
        "pred_percept = percept_CIFAR.predict(X_train_CIFAR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_percept, y_train_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBHDAU7aPIsl"
      },
      "source": [
        "### Validate Perceptron on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV2xMorZPIsn",
        "outputId": "1879c883-1ae8-4a0e-fec6-fd0fbbc62641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The validation accuracy is given by: 35.360000\n"
          ]
        }
      ],
      "source": [
        "pred_percept = percept_CIFAR.predict(X_val_CIFAR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_percept, y_val_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKVyOFlOPIsn"
      },
      "source": [
        "### Test Perceptron on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Cp8VpkSPIso",
        "outputId": "f2d45722-053e-4441-9810-8efc33d1b8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The testing accuracy is given by: 35.510000\n"
          ]
        }
      ],
      "source": [
        "pred_percept = percept_CIFAR.predict(X_test_CIFAR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_percept, y_test_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOZvUfICPIso"
      },
      "source": [
        "### Perceptron_CIFAR Kaggle Submission\n",
        "\n",
        "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LaucXLW8PIso"
      },
      "outputs": [],
      "source": [
        "output_submission_csv('kaggle/perceptron_submission_CIFAR.csv', percept_CIFAR.predict(X_test_CIFAR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqUqT9inPIsp"
      },
      "source": [
        "# Support Vector Machines (with SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb__bVsAPIsp"
      },
      "source": [
        "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
        "\n",
        "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
        "\n",
        "SVM optimized with SGD has 3 hyperparameters that you can experiment with:\n",
        "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
        "- **Epochs** - similar to as defined above in Perceptron.\n",
        "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zwluPtCPIsq"
      },
      "source": [
        "You will implement the SVM using SGD in the **models/svm.py**\n",
        "\n",
        "The following code: \n",
        "- Creates an instance of the SVM classifier class \n",
        "- The train function of the SVM class is trained on the training data\n",
        "- We use the predict function to find the training accuracy as well as the testing accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0eKzyHPIsq"
      },
      "source": [
        "## Train SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Support Vector Machine (SVM) model.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class SVM:\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\n",
        "        \"\"\"Initialize a new classifier.\n",
        "\n",
        "        Parameters:\n",
        "            n_class: the number of classes\n",
        "            lr: the learning rate\n",
        "            epochs: the number of epochs to train for\n",
        "            reg_const: the regularization constant\n",
        "        \"\"\"\n",
        "        self.w = None  # TODO: change this\n",
        "        self.alpha = lr\n",
        "        self.epochs = epochs\n",
        "        self.reg_const = reg_const\n",
        "        self.n_class = n_class\n",
        "\n",
        "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculate gradient of the svm hinge loss.\n",
        "\n",
        "        Inputs have dimension D, there are C classes, and we operate on\n",
        "        mini-batches of N examples.\n",
        "\n",
        "        Parameters:\n",
        "            X_train: a numpy array of shape (N, D) containing a mini-batch\n",
        "                of data\n",
        "            y_train: a numpy array of shape (N,) containing training labels;\n",
        "                y[i] = c means that X[i] has label c, where 0 <= c < C\n",
        "\n",
        "        Returns:\n",
        "            the gradient with respect to weights w; an array of the same shape\n",
        "                as w\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        loss , Number_of_rows, Number_classes , gradient = 0 , X_train.shape[0],self.w.shape[1] , np.zeros(self.w.shape)\n",
        "        y_traintemp = y_train.astype(int)\n",
        "        output = X_train.dot(self.w)\n",
        "\n",
        "        for index,output in enumerate(output):\n",
        "          correct_outputscore = output[y_traintemp[index]]\n",
        "          gradient +=  (self.reg_const/Number_of_rows ) * self.w\n",
        "\n",
        "          for j in range(Number_classes):\n",
        "            difference = output[j]-correct_outputscore + 1\n",
        "            if j != y_traintemp[index] and difference > 0:\n",
        "              loss += max([0.0,difference])\n",
        "              gradient[:,j] -= X_train[index].T\n",
        "              gradient[:,y_traintemp[index]] += X_train[index].T\n",
        "        \n",
        "        loss /= Number_of_rows\n",
        "        gradient /= Number_of_rows\n",
        "\n",
        "        loss += self.reg_const * np.sum(self.w * self.w)\n",
        "\n",
        "        return loss,gradient\n",
        "    \n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        \"\"\"Train the classifier.\n",
        "\n",
        "        Hint: operate on mini-batches of data for SGD.\n",
        "\n",
        "        Parameters:\n",
        "            X_train: a numpy array of shape (N, D) containing training data;\n",
        "                N examples with D dimensions\n",
        "            y_train: a numpy array of shape (N,) containing training labels\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        length_of_train,Dimensions = X_train.shape\n",
        "        Number_of_classes = 10\n",
        "        self.w = 0.001 * np.random.randn(Dimensions,Number_of_classes)\n",
        "\n",
        "        loss_array = []\n",
        "\n",
        "        for p in range(self.epochs):\n",
        "          if p%4 == 0: \n",
        "            self.alpha /= 100\n",
        "          loss = 0\n",
        "          arr = np.hstack((X_train,y_train.reshape(-1,1)))\n",
        "          np.random.shuffle(arr)\n",
        "          X,Y = arr[:,:-1],arr[:,-1]\n",
        "          start,end = 0,32\n",
        "\n",
        "          for i in range(int(length_of_train/32)):\n",
        "            x_batch,y_batch = X[start:end,],Y[start:end,]\n",
        "            temploss,gradient = self.calc_gradient(x_batch,y_batch)\n",
        "            loss += temploss\n",
        "            self.w += self.alpha * gradient\n",
        "            start+=32\n",
        "            end+=32\n",
        "\n",
        "          loss_array.append(loss)\n",
        "\n",
        "        return loss_array\n",
        "\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\n",
        "\n",
        "        Parameters:\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\n",
        "                N examples with D dimensions\n",
        "\n",
        "        Returns:\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\n",
        "                length N, where each element is an integer giving the predicted\n",
        "                class.\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        return   np.argmax(np.dot(self.w.T,X_test.T),axis=0)\n",
        "\n",
        "\n",
        "# lr = 0.0000001\n",
        "# n_epochs = 20\n",
        "# reg_const = .001\n",
        "\n",
        "lr = 0.0001\n",
        "n_epochs = 8\n",
        "reg_const = .001\n",
        "\n",
        "svm_CIFAR = SVM(n_class_CIFAR, lr, n_epochs, reg_const)\n",
        "loss_array = svm_CIFAR.train(X_train_CIFAR, y_train_CIFAR)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(loss_array)\n",
        "\n",
        "# loss average with 8 epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "P59ZBNfVP_sc",
        "outputId": "f75ccadc-2938-4f73-bebb-eadc6468d9c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa41ccf4bb0>]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiO3pMsUPIsr",
        "outputId": "bc5ad522-5e03-4956-a0b9-673011331b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training accuracy is given by: 40.807500\n"
          ]
        }
      ],
      "source": [
        "pred_svm = svm_CIFAR.predict(X_train_CIFAR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_svm, y_train_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0t0auHiPIsr"
      },
      "source": [
        "### Validate SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHM1b7rNPIsr",
        "outputId": "16d0040e-e94e-4fcb-fd67-f6b0951bf0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The validation accuracy is given by: 36.460000\n"
          ]
        }
      ],
      "source": [
        "pred_svm = svm_CIFAR.predict(X_val_CIFAR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_svm, y_val_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqGmh8CkPIss"
      },
      "source": [
        "### Test SVM on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrDM8q8APIss",
        "outputId": "b1f74216-c2e1-462a-fa80-4fbe7c950304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The testing accuracy is given by: 35.630000\n"
          ]
        }
      ],
      "source": [
        "pred_svm = svm_CIFAR.predict(X_test_CIFAR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_svm, y_test_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQUPCX7PIss"
      },
      "source": [
        "### SVM_CIFAR Kaggle Submission\n",
        "\n",
        "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9SJ74yEaPIss"
      },
      "outputs": [],
      "source": [
        "output_submission_csv('kaggle/svm_submission_CIFAR.csv', svm_CIFAR.predict(X_test_CIFAR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpe_KrlpPIst"
      },
      "source": [
        "# Softmax Classifier (with SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "oHKm9wh7PIst"
      },
      "source": [
        "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
        "\n",
        "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
        "\n",
        "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N21mDPKmPIsu"
      },
      "source": [
        "The softmax classifier has 3 hyperparameters that you can experiment with:\n",
        "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
        "- **Number of Epochs** - As described for perceptron.\n",
        "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK13eNjUPIsu"
      },
      "source": [
        "You will implement a softmax classifier using SGD in the **models/softmax.py**\n",
        "\n",
        "The following code: \n",
        "- Creates an instance of the Softmax classifier class \n",
        "- The train function of the Softmax class is trained on the training data\n",
        "- We use the predict function to find the training accuracy as well as the testing accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd38SUTWPIsv"
      },
      "source": [
        "## Train Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Softmax model.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Softmax:\n",
        "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\n",
        "        \"\"\"Initialize a new classifier.\n",
        "\n",
        "        Parameters:\n",
        "            n_class: the number of classes\n",
        "            lr: the learning rate\n",
        "            epochs: the number of epochs to train for\n",
        "            reg_const: the regularization constant\n",
        "        \"\"\"\n",
        "        self.w = None  # TODO: change this\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.reg_const = reg_const\n",
        "        self.n_class = n_class\n",
        "     \n",
        "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Calculate gradient of the softmax loss.\n",
        "\n",
        "        Inputs have dimension D, there are C classes, and we operate on\n",
        "        mini-batches of N examples.\n",
        "\n",
        "        Parameters:\n",
        "            X_train: a numpy array of shape (N, D) containing a mini-batch\n",
        "                of data\n",
        "            y_train: a numpy array of shape (N,) containing training labels;\n",
        "                y[i] = c means that X[i] has label c, where 0 <= c < C\n",
        "\n",
        "        Returns:\n",
        "            gradient with respect to weights w; an array of same shape as w\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        y_train = y_train.astype(int)\n",
        "\n",
        "        loss , Number_of_rows, Number_classes , gradient = 0 , X_train.shape[0],self.w.shape[1] , np.zeros(self.w.shape)\n",
        "        y_traintemp = y_train.astype(int)\n",
        "\n",
        "        output = X_train.dot(self.w)\n",
        "\n",
        "        for index,row in enumerate(output):\n",
        "          row -= max(row)\n",
        "          probability = np.exp(row)/np.sum(np.exp(row))\n",
        "          loss += -np.log(probability[int(y_train[index])])\n",
        "          probability[int(y_train[index])] -= 1\n",
        "          gradient += np.dot(X_train[index].reshape(-1,1),probability.reshape(-1,1).reshape(1,-1)) \n",
        "        \n",
        "        loss /= Number_of_rows\n",
        "        gradient /= Number_of_rows\n",
        "\n",
        "        loss += self.reg_const * np.sum(self.w * self.w)\n",
        "        \n",
        "        return loss,gradient\n",
        "    \n",
        "        \n",
        "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
        "        \"\"\"Train the classifier.\n",
        "\n",
        "        Hint: operate on mini-batches of data for SGD.\n",
        "\n",
        "        Parameters:\n",
        "            X_train: a numpy array of shape (N, D) containing training data;\n",
        "                N examples with D dimensions\n",
        "            y_train: a numpy array of shape (N,) containing training labels\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        length_of_train,Dimensions = X_train.shape\n",
        "        Number_of_classes = 10\n",
        "        self.w = 0.001 * np.random.randn(Dimensions,Number_of_classes)\n",
        "\n",
        "        loss_array = []\n",
        "\n",
        "        for p in range(self.epochs):\n",
        "          if p%4 == 0: \n",
        "            self.lr /= 100\n",
        "          loss = 0\n",
        "\n",
        "          arr = np.hstack((X_train,y_train.reshape(-1,1)))\n",
        "          np.random.shuffle(arr)\n",
        "          X,Y = arr[:,:-1],arr[:,-1]\n",
        "          start,end = 0,32\n",
        "\n",
        "          for i in range(int(length_of_train/32)):\n",
        "            x_batch,y_batch = X[start:end,],Y[start:end,]\n",
        "            temploss,gradient = self.calc_gradient(x_batch,y_batch)\n",
        "            loss+=temploss\n",
        "            self.w -= self.lr * gradient\n",
        "            start+=32\n",
        "            end+=32\n",
        "          loss_array.append(loss)\n",
        "        return loss_array\n",
        "\n",
        "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Use the trained weights to predict labels for test data points.\n",
        "\n",
        "        Parameters:\n",
        "            X_test: a numpy array of shape (N, D) containing testing data;\n",
        "                N examples with D dimensions\n",
        "\n",
        "        Returns:\n",
        "            predicted labels for the data in X_test; a 1-dimensional array of\n",
        "                length N, where each element is an integer giving the predicted\n",
        "                class.\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "\n",
        "        return np.argmax(np.dot(self.w.T,X_test.T),axis=0)\n"
      ],
      "metadata": {
        "id": "xjTA9B2OQXvV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "q8cv2Oc1PIsv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "59c2e6eb-8d75-42aa-8436-ed3913954e3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa40e182730>]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3iU9Z338fc3MzkAIclEIockJGhBxROQcLB27W5t1Vq7dHtSq5R6KNpqC33cdtXu1n226/W429aq21ZLgWpXWrTqdnl2aVl03bY8VUxABAIKERCCILFAOOf4ff6YGx0pgUmYcGdmPq/rmiv3/Oaeme996fW5b373b34/c3dERCQ75IRdgIiInDoKfRGRLKLQFxHJIgp9EZEsotAXEcki0bALOJ4hQ4Z4dXV12GWIiKSV5cuXv+3uZcd6rV+HfnV1NfX19WGXISKSVszsje5eU/eOiEgWUeiLiGQRhb6ISBZR6IuIZBGFvohIFlHoi4hkEYW+iEgWycjQ33OwjYee28CabS1hlyIi0q/06x9n9VYkx3jg2fV0uXNeeXHY5YiI9BsZeaU/uCCXs4YVUb95d9iliIj0KxkZ+gC1VTFe3rKbjs6usEsREek3Mjf0q2McaOvk1R37wi5FRKTfyODQLwVg+Rvq4hEROSJjQ7+8ZADDiwuo27wr7FJERPqNE4a+mVWa2fNmttbMGsxsZtB+oZm9YGarzez/mllRwnvuMrNGM3vNzC5PaL8iaGs0szv75pDeVVMV05W+iEiCZK70O4A73H0sMAW4zczGAnOAO939fODfgK8DBK9dA5wLXAH8yMwiZhYBfgh8FBgLXBvs22dqq2JsbznMtj2H+vJrRETSxglD3923u/uKYHsfsA4oB8YAvwt2WwJ8KtieCixw91Z33wQ0ApOCR6O7b3T3NmBBsG+fOdKvX68uHhERoId9+mZWDYwHlgENvBvanwEqg+1yYGvC25qCtu7aj/6OGWZWb2b1zc3NPSnvT5w9bDCD8iIary8iEkg69M2sEHgamOXue4EbgS+b2XJgMNCWioLcfba717p7bVnZMZd4TFo0ksP4kTHq1a8vIgIkGfpmlks88Oe7+zMA7v6qu1/m7jXAL4DXg9238e5VP0BF0NZde5+qqYrx2o697D3c3tdfJSLS7yUzeseAucA6d78/of304G8O8LfAI8FLC4FrzCzfzEYBo4GXgDpgtJmNMrM84jd7F6byYI5lYnUpXQ4vb9nT118lItLvJXOlfzEwDfiQma0MHlcSH32zHngVeBP4KYC7NwBPAmuB3wC3uXunu3cAtwOLid8MfjLYt0+NG1lCjsFy3cwVETnxLJvuvhSwbl5+sJv33Avce4z2RcCinhR4sgrzo5wzvEj9+iIiZPAvchNNrC5l5dY9tGvyNRHJclkR+jVVMQ62dbJu+96wSxERCVVWhH5tdQxA4/VFJOtlRegPLx5AeckAzcMjIlkvK0If4lf7dZt34e5hlyIiEprsCf2qGDv3tdK0W5OviUj2yprQr6kKJl97Q+P1RSR7ZU3onzVsMIPzo7qZKyJZLWtCP5JjjK+KKfRFJKtlTehDvF9//c59tBzS5Gsikp2yLvTdYcUWXe2LSHbKqtAfN7KESI6xXF08IpKlsir0B+ZFOXdEEXWacVNEslRWhT7E5+F5pUmTr4lIdsq60K+tKuVwexcNb2ryNRHJPtkX+u9MvqYuHhHJPlkX+kOLCqgsHaDx+iKSlZJZI7fSzJ43s7Vm1mBmM4P2cWb2YrB8Yr2ZTQrazcweMrNGM1tlZhMSPmu6mW0IHtP77rCOr7aqlPo3dmvyNRHJOslc6XcAd7j7WGAKcJuZjQX+Gfjf7j4O+FbwHOCjxBdDHw3MAB4GMLNS4B5gMjAJuMfMYik8lqTVVMV4e38rW3YdDOPrRURCc8LQd/ft7r4i2N5HfFHzcsCBomC3YuKLowNMBX7mcS8CJWY2HLgcWOLuu9x9N7AEuCKlR5OkidXxydfq1MUjIlmmR336ZlYNjAeWAbOA75jZVuC7wF3BbuXA1oS3NQVt3bWfcqNPL6SoIMpyzbgpIlkm6dA3s0LgaWCWu+8FvgR8zd0rga8Bc1NRkJnNCO4R1Dc3N6fiI/9ETo4xQZOviUgWSir0zSyXeODPd/dngubpwJHtXxLvpwfYBlQmvL0iaOuu/T3cfba717p7bVlZWbLH0WO1VTE27NzPnoNtffYdIiL9TTKjd4z4Vfw6d78/4aU3gQ8G2x8CNgTbC4HPB6N4pgAt7r4dWAxcZmax4AbuZUFbKGqDfn2tmysi2SSaxD4XA9OA1Wa2Mmi7G/gi8KCZRYHDxEfqACwCrgQagYPADQDuvsvMvg3UBfv9g7uH1ql+YUUJ0Ryj/o3dXHrO0LDKEBE5pU4Y+u6+FLBuXq45xv4O3NbNZ80D5vWkwL4yIC/CueXFmnFTRLJK1v0iN1FtVYyVTXto7egMuxQRkVMiq0N/YnWMto4u1mzT5Gsikh2yOvRrqo7czNV4fRHJDlkd+mWD86k6baDG64tI1sjq0If45GvLNfmaiGQJhX51jD8eaGPT2wfCLkVEpM8p9KuCRVX0Iy0RyQJZH/pnlhVSPCBX4/VFJCtkfejn5Bi1VTHqNIJHRLJA1oc+QE11jI3NB9h1QJOviUhmU+gTH8EDmnxNRDKfQh+4oKKY3IhRv1ldPCKS2RT6QEFuhPPLizWCR0QynkI/UFtdyuqmFg63a/I1EclcCv1ATVWMts4u1mxrCbsUEZE+o9APHPmRVp3G64tIBlPoB04rzOeMIYM046aIZDSFfoKaqpgmXxORjJbMwuiVZva8ma01swYzmxm0P2FmK4PH5oT1czGzu8ys0cxeM7PLE9qvCNoazezOvjmk3qutjrH7YDuvN2vyNRHJTMksjN4B3OHuK8xsMLDczJa4+9VHdjCz7wEtwfZY4BrgXGAE8KyZjQl2/SHwEaAJqDOzhe6+NnWHc3Jqq+M/0qrfvIv3nV4YcjUiIql3wit9d9/u7iuC7X3AOqD8yOtmZsBngV8ETVOBBe7e6u6bgEZgUvBodPeN7t4GLAj27TfOGDKI0kF5Gq8vIhmrR336ZlYNjAeWJTT/GfCWu28InpcDWxNebwraums/+jtmmFm9mdU3Nzf3pLyTZmZMGBnTdAwikrGSDn0zKwSeBma5e+JK4tfy7lX+SXP32e5e6+61ZWVlqfrYpNVWx9j09gGa97We8u8WEelrSYW+meUSD/z57v5MQnsU+CTwRMLu24DKhOcVQVt37f3KxOr4eH1d7YtIJkpm9I4Bc4F17n7/US9/GHjV3ZsS2hYC15hZvpmNAkYDLwF1wGgzG2VmecRv9i5MxUGk0nnlxeRFczReX0QyUjKjdy4GpgGrE4Zl3u3ui4gH93u6dty9wcyeBNYSH/lzm7t3ApjZ7cBiIALMc/eG1BxG6uRHI1ygyddEJEOdMPTdfSlg3bz2hW7a7wXuPUb7ImBRz0o89WqqY8xbuonD7Z0U5EbCLkdEJGX0i9xjmFhVSnun88rWPWGXIiKSUgr9Y6gJJl9TF4+IZBqF/jHEBuVxZtkgjeARkYyj0O/GxOpS6jfvoqtLk6+JSOZQ6HejpirG3sMdNDbvD7sUEZGUUeh3493J19TFIyKZQ6HfjerTBnLaoDzqN+tHWiKSORT63TAzaqtjGsEjIhlFoX8ctVWlbNl1kJ37DoddiohISij0j6PmyORr6tcXkQyh0D+O80YUkx/NoU6hLyIZQqF/HHnRHC6sLNGMmyKSMRT6J1BbFaPhzb0causMuxQRkZOm0D+B2uoYHV3OSk2+JiIZQKF/AhNGBpOvaby+iGQAhf4JlAzMY8zQQo3XF5GMoNBPQk1VKSu27KZTk6+JSJpLZo3cSjN73szWmlmDmc1MeO0rZvZq0P7PCe13mVmjmb1mZpcntF8RtDWa2Z2pP5y+UVsVY9/hDta/tS/sUkRETkoya+R2AHe4+wozGwwsN7MlwFBgKnChu7ea2ekAZjaW+Nq55wIjgGfNbEzwWT8EPgI0AXVmttDd16b2kFJv4pHJ197YzTnDi0KuRkSk9054pe/u2919RbC9D1gHlANfAu5z99bgtZ3BW6YCC9y91d03AY3ApODR6O4b3b0NWBDs2+9Vlg6gbHA+y3UzV0TSXI/69M2sGhgPLAPGAH9mZsvM7LdmNjHYrRzYmvC2pqCtu/ajv2OGmdWbWX1zc3NPyuszZkZtlSZfE5H0l3Tom1kh8DQwy933Eu8aKgWmAF8HnjQzO9mC3H22u9e6e21ZWdnJflzK1FTFaNp9iB0tmnxNRNJXUqFvZrnEA3++uz8TNDcBz3jcS0AXMATYBlQmvL0iaOuuPS2826+vLh4RSV/JjN4xYC6wzt3vT3jpV8BfBPuMAfKAt4GFwDVmlm9mo4DRwEtAHTDazEaZWR7xm70LU3kwfWnsiCIG5Ea0kpaIpLVkRu9cDEwDVpvZyqDtbmAeMM/M1gBtwHR3d6DBzJ4E1hIf+XObu3cCmNntwGIgAsxz94aUHk0fyo3kcGFlMcvVry8iaeyEoe/uS4Hu+uqv7+Y99wL3HqN9EbCoJwX2J7VVpTz829c50NrBoPxkzpciIv2LfpHbA7XVMTo1+ZqIpDGFfg9MqIphhvr1RSRtKfR7oKggl7OGDtYIHhFJWwr9HqqpivHylj2afE1E0pJCv4cmVpeyv7WDV3fsDbsUEZEeU+j3UE1VfFEVDd0UkXSk0O+hitgAhhblU6ebuSKShhT6PWRm1FaXasZNEUlLCv1eqK2K8WbLYd7ccyjsUkREekSh3wu1Ve8uqiIikk4U+r1wzvDBDMyLUK8uHhFJMwr9XohGchg/skS/zBWRtKPQ76WaqlJe3bGXfYfbwy5FRCRpCv1eqq2K0eXw8hZNviYi6UOh30vjR5aQY7qZKyLpRaHfS4MLcjl7WBHLNfmaiKQRhf5JqK2OT77W0dkVdikiIklJZo3cSjN73szWmlmDmc0M2v/ezLaZ2crgcWXCe+4ys0Yze83MLk9ovyJoazSzO/vmkE6dmqoYB9s6Wbd9X9iliIgkJZk1/zqAO9x9hZkNBpab2ZLgte+7+3cTdzazscQXPT8XGAE8GyycDvBD4CNAE1BnZgvdfW0qDiQMtdVHfqS1i/MrikOuRkTkxE54pe/u2919RbC9D1gHlB/nLVOBBe7e6u6bgEZgUvBodPeN7t4GLAj2TVvlJQMYUVygm7kikjZ61KdvZtXAeGBZ0HS7ma0ys3lmFgvayoGtCW9rCtq6az/6O2aYWb2Z1Tc3N/ekvFDUVJdSv3kX7lpURUT6v6RD38wKgaeBWe6+F3gYOBMYB2wHvpeKgtx9trvXunttWVlZKj6yT9VWxXhrbytNuzX5moj0f0mFvpnlEg/8+e7+DIC7v+Xune7eBfyEePcNwDagMuHtFUFbd+1prbZai6qISPpIZvSOAXOBde5+f0L78ITd/gpYE2wvBK4xs3wzGwWMBl4C6oDRZjbKzPKI3+xdmJrDCM/Zw4oozI9qsXQRSQvJjN65GJgGrDazlUHb3cC1ZjYOcGAzcAuAuzeY2ZPAWuIjf25z904AM7sdWAxEgHnu3pDCYwlFJMc0+ZqIpI0Thr67LwXsGC8tOs577gXuPUb7ouO9L13VVpXywHPrWbOthfPKNXRTRPov/SI3Ba6dVMmI4gF8ft5LNO7UD7VEpP9S6KfA6UUFzL95MpEc47o5y9i662DYJYmIHJNCP0WqhwziX2+axOH2Lj4350V2tBwOuyQRkT+h0E+hs4cV8diNk9i1v43r5y5j14G2sEsSEXkPhX6KjassYc70iWzddZDp815ir1bWEpF+RKHfBy468zQeub6Gddv3ctOjdRxq6wy7JBERQKHfZ/7i7NN54JpxLH9jN7c8vpzWDgW/iIRPod+HrrpgBPd98gJ+t76ZWQtWarEVEQmdQr+PfXZiJX931Vh+vWYHf/P0arq6NBuniIQnmWkY5CTd9IFR7D/cwfefXU9hfoS//8tziU9pJCJyain0T5GvXvo+9re285Pfb6KwIMrXLz877JJEJAsp9E8RM+PuK89hf2sHP3z+dQrzc/nSn58ZdlkikmUU+qeQmfGPnzif/a2d/NNvXqUwP8K0i6rDLktEsohC/xSL5Bj3f/ZCDrV18Hf/3kBhQZS/Gl8RdlkikiU0eicEuZEcfvC5Cbz/zNP461+uYnHDjrBLEpEsodAPSUFuhJ98vpYLKor5ys9f5vcb+v8i8CKS/hT6IRqUH+XRL0zijLJBzPjZcuo3a8lFEelbyayRW2lmz5vZWjNrMLOZR71+h5m5mQ0JnpuZPWRmjWa2yswmJOw73cw2BI/pqT+c9FM8MJd/vWkyw4oLuOHROtZsawm7JBHJYMlc6XcAd7j7WGAKcJuZjYX4CQG4DNiSsP9HiS+GPhqYATwc7FsK3ANMBiYB95hZLEXHkdbKBufz+M2TKSrI1epbItKnThj67r7d3VcE2/uAdUB58PL3gW8QXxz9iKnAzzzuRaDEzIYDlwNL3H2Xu+8GlgBXpO5Q0lt5yQAev3kyOWZcP+clrb4lIn2iR336ZlYNjAeWmdlUYJu7v3LUbuXA1oTnTUFbd+1Hf8cMM6s3s/rm5uy6uTkqWH3rUHsn181Zxlt7tfqWiKRW0qFvZoXA08As4l0+dwPfSnVB7j7b3WvdvbasrCzVH9/vnTO8iEdvmMgf97dy/RytviUiqZVU6JtZLvHAn+/uzwBnAqOAV8xsM1ABrDCzYcA2oDLh7RVBW3ftcpTxI2PMmT6RN4LVt/Zp9S0RSZFkRu8YMBdY5+73A7j7anc/3d2r3b2aeFfNBHffASwEPh+M4pkCtLj7dmAxcJmZxYIbuJcFbXIMF515Gg9fNyG++tZj9Vp9S0RSIpkr/YuBacCHzGxl8LjyOPsvAjYCjcBPgC8DuPsu4NtAXfD4h6BNunHpOUP5/tXjqNu8i1sfX05bhxZhEZGTY+79d1GP2tpar6+vD7uM0D1Rt4W/eXo1Hz1vGP9y7XiiEf2mTkS6Z2bL3b32WK8pPdLA1RNH8rcfO4dfr9nBnc9o9S0R6T3Nspkmbv6zM9jf2sEDz26gMD/KPR8fq9W3RKTHFPppZOalo9l/uIM5SzfR3tnF1y8/i5KBeWGXJSJpRKGfRsyMb37sHByY9/82sXDlm8y45Axu/MAoBuXrP6WInJj69NOMmfF3V43lNzMv4aIzT+N7S9ZzyT8/z7ylmzjcrmGdInJ8Gr2T5l7espvvLH6NP7z+R0YUFzDrw2P45IRyjfARyWIavZPBxo+M8fMvTmH+zZMpKyrgG0+v4rIHfsd/rtquUT4i8icU+hni4vcN4Vdffj8/nlZDNMe47ecr+PgPlvI/r+2kP/9rTkROLYV+BjEzLj93GL+eeQn3f/ZC9h5u5ws/rePqH79InVblEhHUp5/R2jq6eKJ+Kw89t4Hmfa38xVll/PXlZ3HuiOKwSxORPnS8Pn2FfhY41NbJYy9s5uH/eZ2WQ+1cdcFw/tdHxnBGWWHYpYlIH1DoCwAth9qZ8/uNzF26idaOLj5TU8FXLx3NiJIBYZcmIimk0Jf3aN7Xyo/+p5H5L24Bg2lTqvjyn5/JaYX5YZcmIimg0Jdjatp9kIee28BTy5sYkBvhpg+M4uZLzqCoIDfs0kTkJCj05bgad+7n+0vW85+rt1MyMJcvffBMpr+/moLcSNiliUgvKPQlKWu2tfCdxa/x2/XNDC3K5ysfGs3VEyvJ1a97RdKKfpErSTmvvJjHbpzEEzOmUBkbyN/+ag2Xfu+3/Orlbfp1r0iGSGaN3Eoze97M1ppZg5nNDNq/bWarguUT/8vMRgTtZmYPmVlj8PqEhM+abmYbgsf0vjssORmTzziNX956ET/9wkQG5UeZ9cRKvrLgZS3XKJIBTti9Y2bDgeHuvsLMBgPLgU8ATe6+N9jnq8BYd781WD/3K8CVwGTgQXefbGalQD1QC3jwOTXuvru771b3Tvi6upzZv9/Ifb9+lQ+OKeOR62sYkKe+fpH+7KS6d9x9u7uvCLb3AeuA8iOBHxhEPMgBpgI/87gXgZLgxHE5sMTddwVBvwS4otdHJadETo5x6wfP5J8+dT6/39DMtLnLaDnUHnZZItJLPerTN7NqYDywLHh+r5ltBa4DvhXsVg5sTXhbU9DWXfvR3zHDzOrNrL65ubkn5UkfunriSH7wuQm80rSHa2a/SPO+1rBLEpFeSDr0zawQeBqYdeQq392/6e6VwHzg9lQU5O6z3b3W3WvLyspS8ZGSIleeP5y50yey+e0DfOaRP9C0+2DYJYlIDyUV+maWSzzw57v7M8fYZT7wqWB7G1CZ8FpF0NZdu6SRS8aU8fjNk9l1oI1PP/wCjTv3hV2SiPRAMqN3DJgLrHP3+xPaRyfsNhV4NdheCHw+GMUzBWhx9+3AYuAyM4uZWQy4LGiTNFNTFeOJWy6io8v5zCMvsKppT9gliUiSkrnSvxiYBnwoGJ65Mhihc5+ZrTGzVcQDfGaw/yJgI9AI/AT4MoC77wK+DdQFj38I2iQNnTO8iKduvYhB+VE+95NlvPD6H8MuSUSSoF/kyknZ0XKYaXOX8caug/zocxP48NihYZckkvX0i1zpM8OKC3jylos4Z9hgbnl8Of/2clPYJYnIcSj05aTFBuUx/4tTmFRdyteeeIXH/rA57JJEpBsKfUmJwvwoP71hIh8ZO5R7FjbwL89t0ILsIv2QQl9SpiA3wsPXTeCTE8r53pL1/ON/rtNEbSL9TDTsAiSzRCM5fPfTF1JUkMvcpZtoOdTOfZ88n6imZxbpFxT6knI5OcY9Hx9LycBcHnh2A/sPd/DgtePIj2qiNpGw6fJL+oSZMevDY/jWVWP5TcMObny0jgOtHWGXJZL1FPrSp278wCi+95kLeXHjLq6bs4w9B9vCLkkkqyn0pc99qqaCH103gbVv7uXqH7/Izr2Hwy5JJGsp9OWUuPzcYTx6w0Sadh/k04+8wJY/aoZOkTAo9OWUef/7hjD/i1PYe7idTz/yB17boRk6RU41hb6cUuMqS3jyloswg8/++AVe3tLtapki0gcU+nLKjRk6mKdufT8lA3O5bs4ylm54O+ySRLKGQl9CUVk6kF/echEjSwdy46N1/GbNjrBLEskKCn0JzelFBSyYMYVzy4v48vzl/LJ+64nfJCInRaEvoSoZmMf8mydz8fuG8PWnVjF36aawSxLJaAp9Cd3AvChzptfy0fOG8e3/WMv/WbSOxp37NVmbSB844cpZZlYJ/AwYCjgw290fNLPvAB8H2oDXgRvcfU/wnruAm4BO4KvuvjhovwJ4EIgAc9z9vuN9t1bOyi4dnV1889/W8ETQzVOYH+XcEUVcUFHMeeXFXFBRQlXpQHJyLORKRfq3462clUzoDweGu/sKMxsMLAc+AVQA/+3uHWb2TwDu/jdmNhb4BTAJGAE8C4wJPm498BGgifg6ude6+9ruvluhn33cncad+3mlqYXVTXtYta2FtW/upbWjC4DBBVHOLy/m/IpiLigv4YKKYipiAzDTiUDkiOOF/gln2XT37cD2YHufma0Dyt39vxJ2exH4dLA9FVjg7q3AJjNrJH4CAGh0941BUQuCfbsNfck+ZsbooYMZPXQwn66pAKC9s4sNb+1n9bY9rGpqYfW2Fn66dDNtnfETQcnA3PiJoLyYCyqKOb+ihBHFBToRiBxDj6ZWNrNqYDyw7KiXbgSeCLbLiZ8EjmgK2gC2HtU++RjfMQOYATBy5MielCcZKjeSw9gRRYwdUcTVE+NtbR1drH9rX3ASiJ8MZv9uIx3BfYDTBuUF/xqInwQuqChmaFFBiEch0j8kHfpmVgg8Dcxy970J7d8EOoD5qSjI3WcDsyHevZOKz5TMkxfN4bzyeF8/xC8ODrd38uqOffFuoeBfBL/f8DadwYng9MH573YNVRRzfnkJZYPzQzwKkVMvqdA3s1zigT/f3Z9JaP8CcBVwqb97c2AbUJnw9oqgjeO0i5y0gtwI4ypLGFdZ8k7bobZO1m7f+879gdVNLfz3azs58n/r8OICqk4bSGF+lIF5UQblRynMjzAoP8qg4Pmg/Mg724X5UQbmRyjMjz8fmBvRjWVJKycMfYt3jM4F1rn7/QntVwDfAD7o7olTJi4Efm5m9xO/kTsaeAkwYLSZjSIe9tcAn0vVgYgcy4C8CDVVMWqqYu+0HWjtoOHNvaxq2sPqbS1s33OYN/cc5kBbBwdaOzjQ2smh9s6kv2Ng3pGTRPA3YfvIyeTIiWRgfpSCaA75uRHyoznBI0JesF2Q+97nR7YjOrFIiiRzpX8xMA1YbWYrg7a7gYeAfGBJcMPsRXe/1d0bzOxJ4jdoO4Db3L0TwMxuBxYTH7I5z90bUno0IkkYlB9l0qhSJo0q7Xafzi7nQFsHB1s72d8anAza4ieEd7c72N/aycHg+ZHt/a0dNO9vZfMfDwYnkQ4OtCV/EjmW3IiRF3nvySIvOCnkR3PIz82Jvx6NkJ/73hNGXjSH3EgOeRF7dzv4m//OaznkRuN/86JGXiRCbjT+ne/ZL/ibGzHdKE9TJxyyGSYN2ZRM0dXlHGqPnzBaO7po7ejkcHsXbZ1dtLbHn8fbu2gLXo+3JzwP/rYF+yW+7522o5+3d9Le6e+MdEqlvCD8E08kkRyjN6eC3p5Aen3a6eUbe/t9vTm+c4YX8S/Xju/t9/V+yKaInLycHHun6ycM7v5O+Ld3xE82bcHf9s4u2jucts5O2jreu097Z/zk0R7s3/7O+/yd5+0Jn9XZi19R9/a6s7eXq7290O315XEv31gZG9Dbbzwuhb5IFjCzeLdNNCfeKStZS3PviIhkEYW+iEgWUeiLiGQRhb6ISBZR6IuIZBGFvohIFlHoi4hkEYW+iEgW6dfTMJhZM/DGSXzEEODtFJXT3+jY0lcmH5+OrX+ocveyY73Qr0P/ZJlZfXfzT6Q7HVv6yuTj07H1f+reERHJIgp9EZEskumhPzvsAvqQji19ZfLx6dj6uYzu0xcRkffK9Ct9ERFJoNAXEckiGRn6ZnaFmb1mZo1mdhc4gdcAAALnSURBVGfY9aSSmVWa2fNmttbMGsxsZtg1pZqZRczsZTP7j7BrSSUzKzGzp8zsVTNbZ2YXhV1TKpnZ14L/J9eY2S/MrCDsmnrLzOaZ2U4zW5PQVmpmS8xsQ/A3FmaNvZVxoW9mEeCHwEeBscC1ZjY23KpSqgO4w93HAlOA2zLs+ABmAuvCLqIPPAj8xt3PBi4kg47RzMqBrwK17n4eEAGuCbeqk/IocMVRbXcCz7n7aOC54HnaybjQByYBje6+0d3bgAXA1JBrShl33+7uK4LtfcSDozzcqlLHzCqAjwFzwq4llcysGLgEmAvg7m3uvifcqlIuCgwwsygwEHgz5Hp6zd1/B+w6qnkq8Fiw/RjwiVNaVIpkYuiXA1sTnjeRQaGYyMyqgfHAsnArSakHgG8AXWEXkmKjgGbgp0HX1RwzGxR2Uani7tuA7wJbgO1Ai7v/V7hVpdxQd98ebO8AhoZZTG9lYuhnBTMrBJ4GZrn73rDrSQUzuwrY6e7Lw66lD0SBCcDD7j4eOECadg8cS9C/PZX4yW0EMMjMrg+3qr7j8bHuaTnePRNDfxtQmfC8ImjLGGaWSzzw57v7M2HXk0IXA39pZpuJd8t9yMweD7eklGkCmtz9yL/KniJ+EsgUHwY2uXuzu7cDzwDvD7mmVHvLzIYDBH93hlxPr2Ri6NcBo81slJnlEb+ZtDDkmlLGzIx4v/A6d78/7HpSyd3vcvcKd68m/t/tv909I64W3X0HsNXMzgqaLgXWhlhSqm0BppjZwOD/0UvJoBvVgYXA9GB7OvDvIdbSa9GwC0g1d+8ws9uBxcRHEMxz94aQy0qli4FpwGozWxm03e3ui0KsSZLzFWB+cDGyEbgh5HpSxt2XmdlTwAriI8xeJo2nLTCzXwB/DgwxsybgHuA+4Ekzu4n4lO+fDa/C3tM0DCIiWSQTu3dERKQbCn0RkSyi0BcRySIKfRGRLKLQFxHJIgp9EZEsotAXEcki/x85sV9PY3qO6gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "lr = 0.0001\n",
        "n_epochs = 12\n",
        "reg_const = 0.001\n",
        "\n",
        "softmax_CIFAR = Softmax(n_class_CIFAR, lr, n_epochs, reg_const)\n",
        "l = softmax_CIFAR.train(X_train_CIFAR, y_train_CIFAR)\n",
        "\n",
        "plt.plot(l)\n",
        "\n",
        "\n",
        "# loss average with 8 epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jpuzSVCuPIsw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6a3543-bfb7-4769-d08f-bc96c388dcc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training accuracy is given by: 40.685000\n"
          ]
        }
      ],
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_train_CIFAR)\n",
        "print('The training accuracy is given by: %f' % (get_acc(pred_softmax, y_train_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybZ5XxESPIsw"
      },
      "source": [
        "### Validate Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "kkBrrJg2PIsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30b56579-72d2-4ffd-97f7-6a04629f18a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The validation accuracy is given by: 36.980000\n"
          ]
        }
      ],
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_val_CIFAR)\n",
        "print('The validation accuracy is given by: %f' % (get_acc(pred_softmax, y_val_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfBWY0sKPIsx"
      },
      "source": [
        "### Testing Softmax on CIFAR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Y9YZWZrPPIsy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166fe235-2aec-453f-b3a1-7d03864c1e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The testing accuracy is given by: 37.370000\n"
          ]
        }
      ],
      "source": [
        "pred_softmax = softmax_CIFAR.predict(X_test_CIFAR)\n",
        "print('The testing accuracy is given by: %f' % (get_acc(pred_softmax, y_test_CIFAR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZfqbnhePIsy"
      },
      "source": [
        "### Softmax_CIFAR Kaggle Submission\n",
        "\n",
        "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "-GIsb6Y6PIsz"
      },
      "outputs": [],
      "source": [
        "output_submission_csv('kaggle/softmax_submission_CIFAR.csv', softmax_CIFAR.predict(X_test_CIFAR))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F1_K9HZMNZ-E"
      },
      "execution_count": 25,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "87QaEv8xPIsg",
        "yqUqT9inPIsp"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "7b2df3d65e46e7144579e91ca5e04b28af119ed783d78dfb570448b77ef9e879"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}